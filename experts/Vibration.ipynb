{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:29:46.801458Z","iopub.execute_input":"2026-01-11T08:29:46.801615Z","iopub.status.idle":"2026-01-11T08:29:49.074269Z","shell.execute_reply.started":"2026-01-11T08:29:46.801599Z","shell.execute_reply":"2026-01-11T08:29:49.073512Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport requests\nfrom io import BytesIO\n\n# -------------------------\n# Create local folders\n# -------------------------\nos.makedirs(\"MetroDataset/Failure\", exist_ok=True)\nos.makedirs(\"MetroDataset/Normal\", exist_ok=True)\n\n# -------------------------\n# URLs for the failure ZIPs (raw GitHub links)\n# -------------------------\nurls_failure = {\n    'x': \"https://github.com/EnfangCui/MetroDataset/raw/master/Failure/Metro_vibration_v1_x_axis_failure.zip\",\n    'y': \"https://github.com/EnfangCui/MetroDataset/raw/master/Failure/Metro_vibration_v1_y_axis_failure.zip\",\n    'z': \"https://github.com/EnfangCui/MetroDataset/raw/master/Failure/Metro_vibration_v1_z_axis_failure.zip\"\n}\n\nurls_normal = {\n    'x': \"https://github.com/EnfangCui/MetroDataset/raw/master/Normal/Metro_vibration_v1_x_axis_normal.zip\",\n    'y': \"https://github.com/EnfangCui/MetroDataset/raw/master/Normal/Metro_vibration_v1_y_axis_normal.zip\",\n    'z': \"https://github.com/EnfangCui/MetroDataset/raw/master/Normal/Metro_vibration_v1_z_axis_normal.zip\"\n}\n\n# -------------------------\n# Helper function to download & unzip\n# -------------------------\ndef download_and_unzip(url, save_dir):\n    os.makedirs(save_dir, exist_ok=True)\n    r = requests.get(url)\n    with zipfile.ZipFile(BytesIO(r.content)) as z:\n        z.extractall(save_dir)\n\n# Download & unzip all\nfor axis, url in urls_failure.items():\n    download_and_unzip(url, f\"MetroDataset/Failure/{axis}\")\n\nfor axis, url in urls_normal.items():\n    download_and_unzip(url, f\"MetroDataset/Normal/{axis}\")\n\nprint(\"All files downloaded and extracted successfully!\")\n\n# -------------------------\n# Load CSVs into arrays\n# -------------------------\ndef load_axis(base_folder):\n    axes = ['x', 'y', 'z']  # ensure correct order\n    arrays = []\n    for axis in axes:\n        folder_path = os.path.join(base_folder, axis)\n        files = sorted([f for f in os.listdir(folder_path) if f.endswith('.csv')])\n        axis_data = []\n        for f in files:\n            df = pd.read_csv(os.path.join(folder_path, f))\n            axis_data.append(df.values.flatten())\n        arrays.append(np.concatenate(axis_data))  # 1D array per axis\n    # Stack axes as columns\n    return np.column_stack(arrays)  # shape: [samples, 3]\n\nnormal = load_axis(\"MetroDataset/Normal\")\nfailure = load_axis(\"MetroDataset/Failure\")\n\nprint(\"Normal shape:\", normal.shape)\nprint(\"Failure shape:\", failure.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:30:52.709300Z","iopub.execute_input":"2026-01-11T08:30:52.709598Z","iopub.status.idle":"2026-01-11T08:31:03.398979Z","shell.execute_reply.started":"2026-01-11T08:30:52.709575Z","shell.execute_reply":"2026-01-11T08:31:03.398290Z"}},"outputs":[{"name":"stdout","text":"All files downloaded and extracted successfully!\nNormal shape: (7270400, 3)\nFailure shape: (12462080, 3)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# -------------------------\n# Save the best model\n# -------------------------\n\n# Option 1: Save entire model (architecture + weights + optimizer state)\nmodel.save(\"metro_vibration_model.h5\")\nprint(\"Model saved as 'metro_vibration_model.h5'\")\n\n# Option 2 (optional): Save in TensorFlow SavedModel format\nmodel.save(\"metro_vibration_model_tf\", save_format=\"tf\")\nprint(\"Model saved in TensorFlow SavedModel format as 'metro_vibration_model_tf'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:38:30.394086Z","iopub.status.idle":"2026-01-11T08:38:30.394382Z","shell.execute_reply.started":"2026-01-11T08:38:30.394208Z","shell.execute_reply":"2026-01-11T08:38:30.394223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# Metro Vibration Classification Pipeline (Conv1D + LSTM + Engineered Features)\n# ===============================\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\n\n# ------------------------------\n# Step 0: Load your data\n# ------------------------------\n# normal and failure are already loaded\nX = np.vstack([normal, failure])\ny = np.hstack([np.zeros(normal.shape[0]), np.ones(failure.shape[0])])\nprint(\"Raw X shape:\", X.shape, \"y shape:\", y.shape)\n\n# ------------------------------\n# Step 1: Sliding window with engineered features\n# ------------------------------\ndef extract_features(window):\n    \"\"\"\n    Extract basic features per axis for a window\n    Returns shape: [window_size, 3 + 3] -> original + engineered features\n    \"\"\"\n    # original signals\n    x = window[:, 0]\n    y_ = window[:, 1]\n    z = window[:, 2]\n    \n    # engineered features per axis: mean, std, RMS\n    feats = np.stack([\n        np.mean(x), np.std(x), np.sqrt(np.mean(x**2)),\n        np.mean(y_), np.std(y_), np.sqrt(np.mean(y_**2)),\n        np.mean(z), np.std(z), np.sqrt(np.mean(z**2))\n    ])\n    # repeat across time dimension to concatenate\n    feats_tile = np.tile(feats, (window.shape[0], 1))  # [window_size, 9]\n    return np.hstack([window, feats_tile])  # [window_size, 3+9=12]\n\ndef create_windows_features(X, y, window_size=500, step=200, threshold=0.3):\n    X_windows = []\n    y_windows = []\n    for i in range(0, len(X) - window_size + 1, step):\n        window = X[i:i+window_size]\n        X_windows.append(extract_features(window))\n        if y[i:i+window_size].mean() > threshold:\n            y_windows.append(1)\n        else:\n            y_windows.append(0)\n    return np.array(X_windows), np.array(y_windows)\n\nwindow_size = 500\nstep_size = 200\nthreshold = 0.3\n\nX_windows, y_windows = create_windows_features(X, y, window_size, step_size, threshold)\nprint(\"Windows with features shape:\", X_windows.shape, \"Labels shape:\", y_windows.shape)\n\n# ------------------------------\n# Step 2: Scale features\n# ------------------------------\nnum_windows, ws, num_features = X_windows.shape\nX_flat = X_windows.reshape(num_windows*ws, num_features)\n\nscaler = StandardScaler()\nX_flat_scaled = scaler.fit_transform(X_flat)\nX_windows_scaled = X_flat_scaled.reshape(num_windows, ws, num_features)\n\njoblib.dump(scaler, \"scaler.save\")\nprint(\"Scaler saved as 'scaler.save'\")\n\n# ------------------------------\n# Step 3: Train-test split\n# ------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_windows_scaled, y_windows, test_size=0.2, stratify=y_windows, random_state=42\n)\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n\n# ------------------------------\n# Step 4: Class weights\n# ------------------------------\nclasses = np.unique(y_train)\nclass_weights = compute_class_weight('balanced', classes=classes, y=y_train)\nclass_weights_dict = dict(zip(classes, class_weights))\nprint(\"Class weights:\", class_weights_dict)\n\n# ------------------------------\n# Step 5: Conv1D + LSTM model\n# ------------------------------\nmodel = models.Sequential([\n    layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(window_size, num_features)),\n    layers.Conv1D(64, kernel_size=3, activation='relu'),\n    layers.MaxPooling1D(pool_size=2),\n    layers.LSTM(64, return_sequences=False),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# ------------------------------\n# Step 6: Train model\n# ------------------------------\nearly_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\ncheckpoint = callbacks.ModelCheckpoint(\"best_metro_cnn_lstm.keras\", save_best_only=True)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=20,\n    batch_size=512,\n    class_weight=class_weights_dict,\n    callbacks=[early_stop, checkpoint],\n    verbose=2\n)\n\n# ------------------------------\n# Step 7: Evaluate model\n# ------------------------------\ny_pred_prob = model.predict(X_test)\n# tune threshold for better F1\nbest_thresh = 0.4\ny_pred = (y_pred_prob > best_thresh).astype(int)\n\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"F1 Score:\", f1_score(y_test, y_pred))\n\n# ------------------------------\n# Step 8: Multi-agent CSV inference\n# ------------------------------\ndef predict_csv(model, csv_path, scaler_path=\"scaler.save\", window_size=500, step_size=200, threshold=0.3, best_thresh=0.4):\n    scaler = joblib.load(scaler_path)\n    df = pd.read_csv(csv_path)\n    X = df[['x','y','z']].values\n    \n    # create windows\n    X_windows, _ = create_windows_features(X, np.zeros(len(X)), window_size, step_size, threshold)\n    \n    # scale\n    num_windows, ws, num_features = X_windows.shape\n    X_flat = X_windows.reshape(num_windows*ws, num_features)\n    X_scaled = scaler.transform(X_flat).reshape(num_windows, ws, num_features)\n    \n    # predict\n    preds_prob = model.predict(X_scaled)\n    preds = (preds_prob > best_thresh).astype(int)\n    return preds\n\n# Example usage:\n# model = tf.keras.models.load_model(\"best_metro_cnn_lstm.keras\")\n# preds = predict_csv(model, \"agent_input.csv\")\n# print(\"Predictions per window:\", preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:43:39.038401Z","iopub.execute_input":"2026-01-11T08:43:39.038970Z","iopub.status.idle":"2026-01-11T08:45:24.318871Z","shell.execute_reply.started":"2026-01-11T08:43:39.038929Z","shell.execute_reply":"2026-01-11T08:45:24.318094Z"}},"outputs":[{"name":"stdout","text":"Raw X shape: (19732480, 3) y shape: (19732480,)\nWindows with features shape: (98660, 500, 12) Labels shape: (98660,)\nScaler saved as 'scaler.save'\nX_train: (78928, 500, 12) X_test: (19732, 500, 12)\nClass weights: {0: 1.357037240810151, 1: 0.7917026099865588}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m498\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m1,184\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m496\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m6,208\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m248\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">498</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,184</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">496</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,641\u001b[0m (174.38 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,641</span> (174.38 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,641\u001b[0m (174.38 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,641</span> (174.38 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\n124/124 - 10s - 81ms/step - accuracy: 0.6274 - loss: 0.5395 - val_accuracy: 0.5869 - val_loss: 0.5810\nEpoch 2/20\n124/124 - 5s - 38ms/step - accuracy: 0.6206 - loss: 0.5421 - val_accuracy: 0.6373 - val_loss: 0.5456\nEpoch 3/20\n124/124 - 5s - 38ms/step - accuracy: 0.6222 - loss: 0.5337 - val_accuracy: 0.6358 - val_loss: 0.5356\nEpoch 4/20\n124/124 - 5s - 38ms/step - accuracy: 0.6249 - loss: 0.5288 - val_accuracy: 0.5926 - val_loss: 0.5619\nEpoch 5/20\n124/124 - 5s - 38ms/step - accuracy: 0.6131 - loss: 0.5389 - val_accuracy: 0.6340 - val_loss: 0.5428\nEpoch 6/20\n124/124 - 5s - 38ms/step - accuracy: 0.6312 - loss: 0.5247 - val_accuracy: 0.6304 - val_loss: 0.5356\nEpoch 7/20\n124/124 - 5s - 38ms/step - accuracy: 0.6210 - loss: 0.5316 - val_accuracy: 0.5825 - val_loss: 0.5680\nEpoch 8/20\n124/124 - 5s - 38ms/step - accuracy: 0.6289 - loss: 0.5263 - val_accuracy: 0.6361 - val_loss: 0.5330\nEpoch 9/20\n124/124 - 5s - 38ms/step - accuracy: 0.6312 - loss: 0.5235 - val_accuracy: 0.6356 - val_loss: 0.5299\nEpoch 10/20\n124/124 - 5s - 38ms/step - accuracy: 0.6297 - loss: 0.5237 - val_accuracy: 0.6342 - val_loss: 0.5353\nEpoch 11/20\n124/124 - 5s - 38ms/step - accuracy: 0.6241 - loss: 0.5286 - val_accuracy: 0.6353 - val_loss: 0.5305\nEpoch 12/20\n124/124 - 5s - 38ms/step - accuracy: 0.6271 - loss: 0.5262 - val_accuracy: 0.6321 - val_loss: 0.5378\n\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.50      0.98      0.66      7270\n           1       0.97      0.42      0.59     12462\n\n    accuracy                           0.63     19732\n   macro avg       0.74      0.70      0.63     19732\nweighted avg       0.80      0.63      0.62     19732\n\nConfusion Matrix:\n [[7132  138]\n [7184 5278]]\nF1 Score: 0.5904463586530933\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\n\n# mean/std per window\nmeans = X_windows.mean(axis=1)  # shape: [num_windows, num_features]\nstds = X_windows.std(axis=1)\ndf_summary = pd.DataFrame(np.hstack([means, stds]), columns=[f\"mean_{i}\" for i in range(X_windows.shape[2])] + [f\"std_{i}\" for i in range(X_windows.shape[2])])\ndf_summary['label'] = y_windows\ndf_summary.groupby('label').describe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:46:23.185640Z","iopub.execute_input":"2026-01-11T08:46:23.186424Z","iopub.status.idle":"2026-01-11T08:46:28.637667Z","shell.execute_reply.started":"2026-01-11T08:46:23.186399Z","shell.execute_reply":"2026-01-11T08:46:28.637026Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"        mean_0                                                     \\\n         count          mean            std      min          25%   \nlabel                                                               \n0      36351.0  9.088023e+05  524674.847865  62.2696  454437.2613   \n1      62309.0  1.557724e+06  899344.669360  62.2758  778862.2830   \n\n                                                  mean_1                ...  \\\n                50%           75%           max    count          mean  ...   \nlabel                                                                   ...   \n0      9.088123e+05  1.363187e+06  1.817512e+06  36351.0  9.088023e+05  ...   \n1      1.557712e+06  2.336562e+06  3.115412e+06  62309.0  1.557724e+06  ...   \n\n             std_10                 std_11                                   \\\n                75%           max    count          mean           std  min   \nlabel                                                                         \n0      8.149073e-09  2.421439e-08  36351.0  7.548222e-09  6.499806e-09  0.0   \n1      1.350418e-08  3.911555e-08  62309.0  1.370600e-08  1.193840e-08  0.0   \n\n                                                               \n                25%           50%           75%           max  \nlabel                                                          \n0      2.240995e-09  5.820766e-09  1.140870e-08  3.445894e-08  \n1      3.841706e-09  1.059379e-08  2.072193e-08  6.146729e-08  \n\n[2 rows x 192 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"8\" halign=\"left\">mean_0</th>\n      <th colspan=\"2\" halign=\"left\">mean_1</th>\n      <th>...</th>\n      <th colspan=\"2\" halign=\"left\">std_10</th>\n      <th colspan=\"8\" halign=\"left\">std_11</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n      <th>count</th>\n      <th>mean</th>\n      <th>...</th>\n      <th>75%</th>\n      <th>max</th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>36351.0</td>\n      <td>9.088023e+05</td>\n      <td>524674.847865</td>\n      <td>62.2696</td>\n      <td>454437.2613</td>\n      <td>9.088123e+05</td>\n      <td>1.363187e+06</td>\n      <td>1.817512e+06</td>\n      <td>36351.0</td>\n      <td>9.088023e+05</td>\n      <td>...</td>\n      <td>8.149073e-09</td>\n      <td>2.421439e-08</td>\n      <td>36351.0</td>\n      <td>7.548222e-09</td>\n      <td>6.499806e-09</td>\n      <td>0.0</td>\n      <td>2.240995e-09</td>\n      <td>5.820766e-09</td>\n      <td>1.140870e-08</td>\n      <td>3.445894e-08</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62309.0</td>\n      <td>1.557724e+06</td>\n      <td>899344.669360</td>\n      <td>62.2758</td>\n      <td>778862.2830</td>\n      <td>1.557712e+06</td>\n      <td>2.336562e+06</td>\n      <td>3.115412e+06</td>\n      <td>62309.0</td>\n      <td>1.557724e+06</td>\n      <td>...</td>\n      <td>1.350418e-08</td>\n      <td>3.911555e-08</td>\n      <td>62309.0</td>\n      <td>1.370600e-08</td>\n      <td>1.193840e-08</td>\n      <td>0.0</td>\n      <td>3.841706e-09</td>\n      <td>1.059379e-08</td>\n      <td>2.072193e-08</td>\n      <td>6.146729e-08</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 192 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nimport xgboost as xgb\nfrom scipy.stats import skew, kurtosis\n\n# -------------------------\n# Parameters\n# -------------------------\nWINDOW_SIZE = 100      # timesteps per window\nMAX_WINDOWS = 50000    # total windows (split equally between Normal & Failure)\nPRINT_EVERY = 1000\nRANDOM_STATE = 42\n\n# -------------------------\n# Load CSVs\n# -------------------------\ndef load_axis_data(folder):\n    all_data = []\n    for axis in ['x','y','z']:\n        axis_path = os.path.join(folder, axis)\n        files = sorted(os.listdir(axis_path))\n        axis_values = []\n        for f in files:\n            df = pd.read_csv(os.path.join(axis_path, f))\n            axis_values.append(df.values.flatten())\n        axis_array = np.concatenate(axis_values)\n        all_data.append(axis_array)\n    return np.vstack(all_data).T  # shape [samples, 3]\n\nprint(\"Loading Normal data...\")\nnormal_data = load_axis_data(\"MetroDataset/Normal\")\nprint(\"Loading Failure data...\")\nfailure_data = load_axis_data(\"MetroDataset/Failure\")\n\nprint(\"Normal shape:\", normal_data.shape, \"Failure shape:\", failure_data.shape)\n\n# -------------------------\n# Windowing and feature extraction\n# -------------------------\ndef extract_features(data, num_windows, window_size, print_every=1000):\n    X_windows = []\n    for i in range(1, num_windows + 1):\n        start = np.random.randint(0, data.shape[0] - window_size)\n        window = data[start:start+window_size, :]\n        features = []\n        for axis in range(window.shape[1]):  # x, y, z\n            axis_data = window[:, axis]\n            features.extend([\n                axis_data.mean(),\n                axis_data.std(),\n                axis_data.min(),\n                axis_data.max(),\n                skew(axis_data),\n                kurtosis(axis_data),\n                np.median(axis_data),\n                np.percentile(axis_data, 25),\n                np.percentile(axis_data, 75),\n                np.ptp(axis_data),                 # max-min\n                np.sum(np.abs(np.diff(axis_data))), # roughness\n                np.mean(np.abs(np.diff(axis_data))) # mean diff\n            ])\n        X_windows.append(features)\n\n        if i % print_every == 0:\n            print(f\"Processed {i} windows...\")\n\n    return np.array(X_windows)\n\nnum_windows_per_class = MAX_WINDOWS // 2\n\nprint(\"Extracting Normal windows...\")\nX_normal = extract_features(normal_data, num_windows_per_class, WINDOW_SIZE, PRINT_EVERY)\ny_normal = np.zeros(num_windows_per_class)\n\nprint(\"Extracting Failure windows...\")\nX_failure = extract_features(failure_data, num_windows_per_class, WINDOW_SIZE, PRINT_EVERY)\ny_failure = np.ones(num_windows_per_class)\n\n# -------------------------\n# Combine and shuffle\n# -------------------------\nX_features = np.vstack([X_normal, X_failure])\ny_windows = np.hstack([y_normal, y_failure])\n\nX_features, y_windows = shuffle(X_features, y_windows, random_state=RANDOM_STATE)\n\nprint(\"Feature matrix shape:\", X_features.shape)\nprint(\"Labels shape:\", y_windows.shape)\n\n# -------------------------\n# Train-test split\n# -------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_features, y_windows, test_size=0.2, stratify=y_windows, random_state=RANDOM_STATE\n)\n\n# Standardize\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# -------------------------\n# Train XGBoost\n# -------------------------\nnum_class0 = np.sum(y_train==0)\nnum_class1 = np.sum(y_train==1)\nscale_pos_weight = num_class0 / num_class1\n\nclf = xgb.XGBClassifier(\n    n_estimators=200,\n    max_depth=6,\n    learning_rate=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    scale_pos_weight=scale_pos_weight,\n    use_label_encoder=False,\n    eval_metric='logloss',\n    random_state=RANDOM_STATE\n)\n\nprint(\"Training XGBoost model...\")\nclf.fit(X_train_scaled, y_train)\n\n# -------------------------\n# Evaluate\n# -------------------------\ny_pred = clf.predict(X_test_scaled)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"F1 Score:\", f1_score(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T08:59:13.282021Z","iopub.execute_input":"2026-01-11T08:59:13.282308Z","iopub.status.idle":"2026-01-11T09:02:40.135836Z","shell.execute_reply.started":"2026-01-11T08:59:13.282287Z","shell.execute_reply":"2026-01-11T09:02:40.134949Z"}},"outputs":[{"name":"stdout","text":"Loading Normal data...\nLoading Failure data...\nNormal shape: (7270400, 3) Failure shape: (12462080, 3)\nExtracting Normal windows...\nProcessed 1000 windows...\nProcessed 2000 windows...\nProcessed 3000 windows...\nProcessed 4000 windows...\nProcessed 5000 windows...\nProcessed 6000 windows...\nProcessed 7000 windows...\nProcessed 8000 windows...\nProcessed 9000 windows...\nProcessed 10000 windows...\nProcessed 11000 windows...\nProcessed 12000 windows...\nProcessed 13000 windows...\nProcessed 14000 windows...\nProcessed 15000 windows...\nProcessed 16000 windows...\nProcessed 17000 windows...\nProcessed 18000 windows...\nProcessed 19000 windows...\nProcessed 20000 windows...\nProcessed 21000 windows...\nProcessed 22000 windows...\nProcessed 23000 windows...\nProcessed 24000 windows...\nProcessed 25000 windows...\nExtracting Failure windows...\nProcessed 1000 windows...\nProcessed 2000 windows...\nProcessed 3000 windows...\nProcessed 4000 windows...\nProcessed 5000 windows...\nProcessed 6000 windows...\nProcessed 7000 windows...\nProcessed 8000 windows...\nProcessed 9000 windows...\nProcessed 10000 windows...\nProcessed 11000 windows...\nProcessed 12000 windows...\nProcessed 13000 windows...\nProcessed 14000 windows...\nProcessed 15000 windows...\nProcessed 16000 windows...\nProcessed 17000 windows...\nProcessed 18000 windows...\nProcessed 19000 windows...\nProcessed 20000 windows...\nProcessed 21000 windows...\nProcessed 22000 windows...\nProcessed 23000 windows...\nProcessed 24000 windows...\nProcessed 25000 windows...\nFeature matrix shape: (50000, 36)\nLabels shape: (50000,)\nTraining XGBoost model...\n\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.94      0.96      0.95      5000\n         1.0       0.96      0.93      0.95      5000\n\n    accuracy                           0.95     10000\n   macro avg       0.95      0.95      0.95     10000\nweighted avg       0.95      0.95      0.95     10000\n\nConfusion Matrix:\n [[4821  179]\n [ 335 4665]]\nF1 Score: 0.9477854530678587\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import joblib\n\n# Save the trained XGBoost model\nmodel_path = \"xgb_metro_model.pkl\"\njoblib.dump(clf, model_path)\nprint(f\"Model saved to {model_path}\")\n\n# Save the StandardScaler too (for scaling new input)\nscaler_path = \"scaler_metro.pkl\"\njoblib.dump(scaler, scaler_path)\nprint(f\"Scaler saved to {scaler_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T09:03:19.441264Z","iopub.execute_input":"2026-01-11T09:03:19.441986Z","iopub.status.idle":"2026-01-11T09:03:19.455866Z","shell.execute_reply.started":"2026-01-11T09:03:19.441937Z","shell.execute_reply":"2026-01-11T09:03:19.454882Z"}},"outputs":[{"name":"stdout","text":"Model saved to xgb_metro_model.pkl\nScaler saved to scaler_metro.pkl\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"","metadata":{}}]}