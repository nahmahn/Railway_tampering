"""
Track Structural Analysis Expert - IMPLEMENTED

Uses vibration analysis with Conv1D + LSTM and XGBoost models for
anomaly detection in railway track structural data.

Based on Vibration.ipynb implementation.

INPUT TYPES:
- CSV files containing:
  - Vibration measurements (x, y, z axes accelerometer data)
  - Geometric Sensor Data (gauge, alignment, cant, curvature, twist)
  - Distributed Acoustic Sensing (DAS) Data

EXPECTED FUNCTIONALITY:
- Load and parse CSV data
- Apply windowing and feature extraction
- Detect anomalies using pre-trained models (XGBoost, CNN-LSTM)
- Calculate risk levels based on predictions
- Generate alerts for anomalies

OUTPUT:
- Return ExpertResult with analysis findings
"""

from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import os
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from scipy.stats import skew, kurtosis

# Optional imports
try:
    import joblib
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False

try:
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import tensorflow as tf
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False

try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except ImportError:
    XGB_AVAILABLE = False


class VibrationAnomalyType(Enum):
    """Types of vibration/structural anomalies."""
    NORMAL = "normal"
    FAILURE = "failure"
    GAUGE_DEVIATION = "gauge_deviation"
    ALIGNMENT_ISSUE = "alignment_issue"
    VIBRATION_ANOMALY = "vibration_anomaly"
    ACOUSTIC_ANOMALY = "acoustic_anomaly"
    RAIL_WEAR = "rail_wear"


@dataclass
class VibrationWindow:
    """A window of vibration data with features."""
    start_idx: int
    end_idx: int
    features: np.ndarray
    prediction: int = 0  # 0 = normal, 1 = failure
    probability: float = 0.0
    label: str = "normal"


@dataclass
class TrackStructuralAnalysisResult:
    """Result structure for track structural analysis."""
    file_path: str
    analysis_status: str
    timestamp: str = ""
    
    # Data info
    sample_count: int = 0
    window_count: int = 0
    data_type: str = ""  # "vibration", "geometric", "das"
    
    # Vibration analysis
    windows_analyzed: List[VibrationWindow] = field(default_factory=list)
    failure_windows: int = 0
    failure_ratio: float = 0.0
    
    # Geometric analysis
    gauge_measurements: List[Dict[str, float]] = field(default_factory=list)
    gauge_deviation_mm: float = 0.0
    alignment_score: float = 0.0
    
    # Wear analysis
    rail_wear_index: float = 0.0
    wear_locations: List[Dict[str, Any]] = field(default_factory=list)
    
    # Vibration statistics
    vibration_stats: Dict[str, Any] = field(default_factory=dict)
    vibration_anomaly_score: float = 0.0
    das_alerts: List[Dict[str, Any]] = field(default_factory=list)
    
    # Model used
    model_used: str = ""
    
    # Overall assessment
    tampering_detected: bool = False
    risk_level: str = "low"  # low, medium, high, critical
    alerts: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    confidence: float = 0.0


class TrackStructuralExpert:
    """
    Track Structural Analysis Expert using vibration analysis.
    """
    
    # Feature extraction parameters from Vibration.ipynb
    WINDOW_SIZE = 100
    STEP_SIZE = 50
    FEATURE_COUNT = 36  # 12 features per axis * 3 axes
    
    def __init__(
        self,
        xgb_model_path: str = None,
        scaler_path: str = None,
        keras_model_path: str = None
    ):
        """Initialize expert with pre-trained models."""
        self.xgb_model = None
        self.scaler = None
        self.keras_model = None
        
        # Try to load XGBoost model
        if xgb_model_path and os.path.exists(xgb_model_path) and JOBLIB_AVAILABLE:
            try:
                self.xgb_model = joblib.load(xgb_model_path)
                print(f"Loaded XGBoost model from {xgb_model_path}")
            except Exception as e:
                print(f"Warning: Could not load XGBoost model: {e}")
        
        # Try to load scaler
        if scaler_path and os.path.exists(scaler_path) and JOBLIB_AVAILABLE:
            try:
                self.scaler = joblib.load(scaler_path)
                print(f"Loaded scaler from {scaler_path}")
            except Exception as e:
                print(f"Warning: Could not load scaler: {e}")
        
        # Try to load Keras model
        if keras_model_path and os.path.exists(keras_model_path) and TF_AVAILABLE:
            try:
                self.keras_model = tf.keras.models.load_model(keras_model_path)
                print(f"Loaded Keras model from {keras_model_path}")
            except Exception as e:
                print(f"Warning: Could not load Keras model: {e}")
        
        # Default prediction threshold
        self.prediction_threshold = 0.4
    
    def extract_features(self, window: np.ndarray) -> np.ndarray:
        """
        Extract statistical features from a window of vibration data.
        
        Features per axis (x, y, z):
        - mean, std, min, max
        - skewness, kurtosis
        - median, 25th percentile, 75th percentile
        - range (max-min)
        - roughness (sum of absolute differences)
        - mean absolute difference
        """
        features = []
        
        for axis in range(window.shape[1]):
            axis_data = window[:, axis]
            features.extend([
                np.mean(axis_data),
                np.std(axis_data),
                np.min(axis_data),
                np.max(axis_data),
                skew(axis_data),
                kurtosis(axis_data),
                np.median(axis_data),
                np.percentile(axis_data, 25),
                np.percentile(axis_data, 75),
                np.ptp(axis_data),  # range
                np.sum(np.abs(np.diff(axis_data))),  # roughness
                np.mean(np.abs(np.diff(axis_data)))  # mean diff
            ])
        
        return np.array(features)
    
    def create_windows(
        self,
        data: np.ndarray,
        window_size: int = None,
        step_size: int = None
    ) -> List[Tuple[int, int, np.ndarray]]:
        """Create sliding windows with feature extraction."""
        window_size = window_size or self.WINDOW_SIZE
        step_size = step_size or self.STEP_SIZE
        
        windows = []
        for i in range(0, len(data) - window_size + 1, step_size):
            window = data[i:i + window_size]
            features = self.extract_features(window)
            windows.append((i, i + window_size, features))
        
        return windows
    
    def predict_xgboost(self, features: np.ndarray) -> Tuple[int, float]:
        """Predict using XGBoost model."""
        if self.xgb_model is None:
            return 0, 0.0
        
        # Scale features if scaler available
        if self.scaler is not None:
            features = self.scaler.transform(features.reshape(1, -1))
        else:
            features = features.reshape(1, -1)
        
        prediction = self.xgb_model.predict(features)[0]
        
        # Get probability if available
        try:
            proba = self.xgb_model.predict_proba(features)[0]
            probability = float(proba[1]) if len(proba) > 1 else float(prediction)
        except:
            probability = float(prediction)
        
        return int(prediction), probability
    
    def detect_data_type(self, df: pd.DataFrame) -> str:
        """Detect the type of CSV data based on columns."""
        columns_lower = [c.lower() for c in df.columns]
        
        # Check for vibration data (x, y, z axes)
        if any(c in columns_lower for c in ['x', 'y', 'z']):
            return "vibration"
        if any('accel' in c for c in columns_lower):
            return "vibration"
        
        # Check for geometric data
        if any(c in columns_lower for c in ['gauge', 'alignment', 'cant', 'curvature', 'twist']):
            return "geometric"
        
        # Check for DAS data
        if any(c in columns_lower for c in ['acoustic', 'das', 'strain']):
            return "das"
        
        # Default to vibration if 3 numeric columns
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) >= 3:
            return "vibration"
        
        return "unknown"
    
    def analyze_csv(
        self,
        file_path: str,
        metadata: Dict[str, Any] = None
    ) -> TrackStructuralAnalysisResult:
        """
        Analyze a CSV file containing track sensor data.
        
        Args:
            file_path: Path to the CSV file
            metadata: Additional metadata
        
        Returns:
            TrackStructuralAnalysisResult with analysis findings
        """
        result = TrackStructuralAnalysisResult(
            file_path=file_path,
            analysis_status="processing",
            timestamp=datetime.utcnow().isoformat() + "Z"
        )
        
        try:
            # Load CSV
            df = pd.read_csv(file_path)
            result.sample_count = len(df)
            
            # Detect data type
            result.data_type = self.detect_data_type(df)
            
            if result.data_type == "vibration":
                result = self._analyze_vibration_data(df, result)
            elif result.data_type == "geometric":
                result = self._analyze_geometric_data(df, result)
            elif result.data_type == "das":
                result = self._analyze_das_data(df, result)
            else:
                # Try vibration analysis as default
                result = self._analyze_vibration_data(df, result)
            
            result.analysis_status = "success"
            
        except FileNotFoundError:
            result.analysis_status = "error"
            result.alerts.append(f"File not found: {file_path}")
        except pd.errors.EmptyDataError:
            result.analysis_status = "error"
            result.alerts.append("CSV file is empty")
        except Exception as e:
            result.analysis_status = "error"
            result.alerts.append(f"Analysis error: {str(e)}")
        
        return result
    
    def _analyze_vibration_data(
        self,
        df: pd.DataFrame,
        result: TrackStructuralAnalysisResult
    ) -> TrackStructuralAnalysisResult:
        """Analyze vibration (accelerometer) data."""
        
        # Get x, y, z columns
        x_col = next((c for c in df.columns if c.lower() == 'x'), None)
        y_col = next((c for c in df.columns if c.lower() == 'y'), None)
        z_col = next((c for c in df.columns if c.lower() == 'z'), None)
        
        if not all([x_col, y_col, z_col]):
            # Try to use first 3 numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns[:3]
            if len(numeric_cols) >= 3:
                x_col, y_col, z_col = numeric_cols[:3]
            else:
                result.alerts.append("Could not identify x, y, z columns")
                return result
        
        data = df[[x_col, y_col, z_col]].values
        
        # Calculate basic statistics
        result.vibration_stats = {
            "x": {
                "mean": float(np.mean(data[:, 0])),
                "std": float(np.std(data[:, 0])),
                "max": float(np.max(data[:, 0])),
                "min": float(np.min(data[:, 0]))
            },
            "y": {
                "mean": float(np.mean(data[:, 1])),
                "std": float(np.std(data[:, 1])),
                "max": float(np.max(data[:, 1])),
                "min": float(np.min(data[:, 1]))
            },
            "z": {
                "mean": float(np.mean(data[:, 2])),
                "std": float(np.std(data[:, 2])),
                "max": float(np.max(data[:, 2])),
                "min": float(np.min(data[:, 2]))
            }
        }
        
        # Create windows and extract features
        windows = self.create_windows(data)
        result.window_count = len(windows)
        
        # Analyze each window
        failure_count = 0
        analyzed_windows = []
        
        for start_idx, end_idx, features in windows:
            # Predict
            if self.xgb_model is not None:
                prediction, probability = self.predict_xgboost(features)
            else:
                # Use statistical heuristics if no model
                prediction, probability = self._heuristic_prediction(features)
            
            window_result = VibrationWindow(
                start_idx=start_idx,
                end_idx=end_idx,
                features=features,
                prediction=prediction,
                probability=probability,
                label="failure" if prediction == 1 else "normal"
            )
            
            analyzed_windows.append(window_result)
            
            if prediction == 1:
                failure_count += 1
        
        result.windows_analyzed = analyzed_windows
        result.failure_windows = failure_count
        result.failure_ratio = failure_count / len(windows) if windows else 0.0
        
        # Calculate anomaly score
        result.vibration_anomaly_score = result.failure_ratio
        
        # Determine risk level and tampering
        result = self._calculate_vibration_risk(result)
        
        result.model_used = "xgboost" if self.xgb_model else "heuristic"
        
        return result
    
    def _analyze_geometric_data(
        self,
        df: pd.DataFrame,
        result: TrackStructuralAnalysisResult
    ) -> TrackStructuralAnalysisResult:
        """Analyze geometric sensor data."""
        
        # Look for gauge column
        gauge_col = next((c for c in df.columns if 'gauge' in c.lower()), None)
        
        if gauge_col:
            gauge_data = df[gauge_col].dropna()
            
            # Standard gauge is 1435mm
            standard_gauge = 1435
            deviation = gauge_data - standard_gauge
            
            result.gauge_deviation_mm = float(np.mean(np.abs(deviation)))
            result.gauge_measurements = [
                {"mean": float(gauge_data.mean())},
                {"std": float(gauge_data.std())},
                {"max_deviation": float(np.max(np.abs(deviation)))}
            ]
        
        # Look for alignment column
        align_col = next((c for c in df.columns if 'alignment' in c.lower()), None)
        
        if align_col:
            align_data = df[align_col].dropna()
            # Score: lower deviation = better (100 - normalized deviation)
            max_acceptable = 10  # mm
            avg_deviation = np.mean(np.abs(align_data))
            result.alignment_score = max(0, 100 - (avg_deviation / max_acceptable * 100))
        
        # Determine risk level
        if result.gauge_deviation_mm > 15:
            result.risk_level = "critical"
            result.tampering_detected = True
            result.alerts.append(f"ðŸš¨ CRITICAL: Gauge deviation {result.gauge_deviation_mm:.1f}mm exceeds limit")
        elif result.gauge_deviation_mm > 10:
            result.risk_level = "high"
            result.alerts.append(f"âš ï¸ HIGH: Gauge deviation {result.gauge_deviation_mm:.1f}mm")
        elif result.gauge_deviation_mm > 5:
            result.risk_level = "medium"
            result.alerts.append(f"âš ï¸ MEDIUM: Gauge deviation {result.gauge_deviation_mm:.1f}mm")
        else:
            result.risk_level = "low"
        
        result.recommendations = self._generate_geometric_recommendations(result)
        result.confidence = 0.85
        
        return result
    
    def _analyze_das_data(
        self,
        df: pd.DataFrame,
        result: TrackStructuralAnalysisResult
    ) -> TrackStructuralAnalysisResult:
        """Analyze Distributed Acoustic Sensing data."""
        
        # DAS data analysis placeholder
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            data = df[col].dropna()
            std = np.std(data)
            threshold = 3 * std  # 3-sigma rule
            
            anomalies = np.abs(data - np.mean(data)) > threshold
            if anomalies.any():
                result.das_alerts.append({
                    "column": col,
                    "anomaly_count": int(anomalies.sum()),
                    "anomaly_ratio": float(anomalies.sum() / len(data))
                })
        
        if result.das_alerts:
            total_anomalies = sum(a["anomaly_count"] for a in result.das_alerts)
            if total_anomalies > 100:
                result.risk_level = "high"
                result.tampering_detected = True
            elif total_anomalies > 50:
                result.risk_level = "medium"
            else:
                result.risk_level = "low"
        
        result.recommendations = ["Review DAS alerts for unusual acoustic patterns"]
        result.confidence = 0.7
        
        return result
    
    def _heuristic_prediction(self, features: np.ndarray) -> Tuple[int, float]:
        """Simple heuristic-based prediction when no model is available."""
        # Use statistical features to detect anomalies
        # Higher variance and extreme values indicate potential issues
        
        # Extract std values (indices 1, 13, 25 for x, y, z)
        std_x = features[1] if len(features) > 1 else 0
        std_y = features[13] if len(features) > 13 else 0
        std_z = features[25] if len(features) > 25 else 0
        
        avg_std = (std_x + std_y + std_z) / 3
        
        # Extract kurtosis values (indices 5, 17, 29)
        kurt_x = features[5] if len(features) > 5 else 0
        kurt_y = features[17] if len(features) > 17 else 0
        kurt_z = features[29] if len(features) > 29 else 0
        
        avg_kurt = abs(kurt_x) + abs(kurt_y) + abs(kurt_z)
        
        # Anomaly score
        anomaly_score = (avg_std * 10 + avg_kurt) / 100
        
        if anomaly_score > 0.5:
            return 1, min(anomaly_score, 1.0)
        return 0, max(0, 1 - anomaly_score)
    
    def _calculate_vibration_risk(
        self,
        result: TrackStructuralAnalysisResult
    ) -> TrackStructuralAnalysisResult:
        """Calculate risk level from vibration analysis."""
        
        if result.failure_ratio > 0.5:
            result.risk_level = "critical"
            result.tampering_detected = True
            result.alerts.append(f"ðŸš¨ CRITICAL: {result.failure_ratio*100:.1f}% of windows show failure patterns")
        elif result.failure_ratio > 0.3:
            result.risk_level = "high"
            result.tampering_detected = True
            result.alerts.append(f"âš ï¸ HIGH: {result.failure_ratio*100:.1f}% of windows show failure patterns")
        elif result.failure_ratio > 0.1:
            result.risk_level = "medium"
            result.alerts.append(f"âš ï¸ MEDIUM: {result.failure_ratio*100:.1f}% of windows show failure patterns")
        else:
            result.risk_level = "low"
            result.alerts.append("âœ… Normal vibration patterns detected")
        
        result.recommendations = self._generate_vibration_recommendations(result)
        result.confidence = 0.8 if self.xgb_model else 0.6
        
        return result
    
    def _generate_vibration_recommendations(
        self,
        result: TrackStructuralAnalysisResult
    ) -> List[str]:
        """Generate recommendations based on vibration analysis."""
        recommendations = []
        
        if result.risk_level == "critical":
            recommendations.extend([
                "Immediately reduce train speed in affected section",
                "Dispatch emergency inspection team",
                "Prepare for potential track closure"
            ])
        elif result.risk_level == "high":
            recommendations.extend([
                "Schedule urgent inspection within 24 hours",
                "Consider speed restrictions",
                "Increase monitoring frequency"
            ])
        elif result.risk_level == "medium":
            recommendations.extend([
                "Schedule routine inspection",
                "Monitor for changes"
            ])
        else:
            recommendations.append("Continue normal operations")
        
        return recommendations
    
    def _generate_geometric_recommendations(
        self,
        result: TrackStructuralAnalysisResult
    ) -> List[str]:
        """Generate recommendations based on geometric analysis."""
        recommendations = []
        
        if result.gauge_deviation_mm > 10:
            recommendations.extend([
                "Immediate gauge correction required",
                "Check for rail fastener failures",
                "Inspect sleeper conditions"
            ])
        elif result.gauge_deviation_mm > 5:
            recommendations.extend([
                "Schedule gauge adjustment",
                "Monitor gauge changes"
            ])
        
        if result.alignment_score < 70:
            recommendations.append("Alignment correction recommended")
        
        if not recommendations:
            recommendations.append("Track geometry within acceptable limits")
        
        return recommendations
    
    def to_dict(self, result: TrackStructuralAnalysisResult) -> Dict[str, Any]:
        """Convert result to dictionary for JSON serialization."""
        return {
            "file_path": result.file_path,
            "analysis_status": result.analysis_status,
            "timestamp": result.timestamp,
            "data_info": {
                "sample_count": result.sample_count,
                "window_count": result.window_count,
                "data_type": result.data_type
            },
            "vibration_analysis": {
                "failure_windows": result.failure_windows,
                "failure_ratio": result.failure_ratio,
                "anomaly_score": result.vibration_anomaly_score,
                "statistics": result.vibration_stats
            },
            "geometric_analysis": {
                "gauge_deviation_mm": result.gauge_deviation_mm,
                "alignment_score": result.alignment_score,
                "measurements": result.gauge_measurements
            },
            "das_alerts": result.das_alerts,
            "result": {
                "tampering_detected": result.tampering_detected,
                "risk_level": result.risk_level,
                "confidence": result.confidence,
                "model_used": result.model_used
            },
            "alerts": result.alerts,
            "recommendations": result.recommendations
        }


# Module-level functions
_expert_instance = None


def get_expert(
    xgb_model_path: str = None,
    scaler_path: str = None
) -> TrackStructuralExpert:
    """Get or create the singleton expert instance."""
    global _expert_instance
    if _expert_instance is None:
        _expert_instance = TrackStructuralExpert(xgb_model_path, scaler_path)
    return _expert_instance


def analyze_csv(
    file_path: str,
    metadata: Dict[str, Any] = None
) -> TrackStructuralAnalysisResult:
    """Analyze a CSV file containing track sensor data."""
    return get_expert().analyze_csv(file_path, metadata)


# ==================== Example Usage ====================

if __name__ == "__main__":
    print("=" * 60)
    print("Track Structural Expert (Vibration) - Test")
    print("=" * 60)
    
    print(f"XGBoost Available: {XGB_AVAILABLE}")
    print(f"Sklearn Available: {SKLEARN_AVAILABLE}")
    print(f"TensorFlow Available: {TF_AVAILABLE}")
    
    expert = TrackStructuralExpert()
    
    # Create sample vibration data
    print("\nCreating sample vibration data...")
    sample_data = pd.DataFrame({
        'x': np.random.normal(0, 0.1, 1000),
        'y': np.random.normal(0, 0.1, 1000),
        'z': np.random.normal(0, 0.1, 1000) + 9.8  # Gravity
    })
    
    sample_path = "./test_vibration.csv"
    sample_data.to_csv(sample_path, index=False)
    
    print(f"\nAnalyzing {sample_path}...")
    result = expert.analyze_csv(sample_path)
    
    print(f"Status: {result.analysis_status}")
    print(f"Data Type: {result.data_type}")
    print(f"Windows Analyzed: {result.window_count}")
    print(f"Failure Ratio: {result.failure_ratio:.2%}")
    print(f"Risk Level: {result.risk_level}")
    print(f"Alerts: {result.alerts}")
    
    # Clean up
    if os.path.exists(sample_path):
        os.remove(sample_path)
